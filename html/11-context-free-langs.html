<HTML>
<head>
<TITLE>SYMBOLIC PROCESSING IN PASCAL - Chapter 11</TITLE>
</head>

<body>

<H1>Context free languages</H1>
<P>
In this chapter we write a small program
which parses strings in accordance with
a grammar.
The program uses the same backtracking technique
as the programs in the preceding two chapters.
<H2>Grammars</H2>
<P>
We return to the topic of formal language theory.
So far we have used grammars for describing the syntax
of the input languages of most of our programs.
Now we shall study grammars and parsing in a more abstract way.
Context free grammars differ from regular expressions
in that they allow defined symbols.
This we encountered already in the macro expander of Chapter 4,
although recursion was not allowed.
In grammars definitions can also be recursive,
and they are now called <EM>productions</EM>.
They allow a string consisting of just a defined symbol on the left
of the production to be replaced by the string on the right
of the production.
Grammars in which the left of a production consists of a single
symbol allow the replacement independently of any context,
so they are called context free.
It is often useful to allow the right side of a production
to consist not just of concatenations
but to allow the various repetition operators of regular expressions.
These are then the (extended) BNF grammars we have been using already.
The program to be written will read a context free grammar
and then determine whether input strings
provided by the user are in the language
defined by the grammar.
<P>
In most languages white space characters such as
blanks, tabs and new lines play a very special role:
they rarely mean anything at all, and at most they serve
as separators. It is interesting to note that in ancient Greek
and in early Latin there were no spaces between words.
If white space characters are to be handled by the grammar explicitly,
then the grammar often becomes quite messy.
So in the program that we shall write,
white space is to be allowed for readable input,
but will not be passed as input to the parser.
<H3>User manual</H3>
<P>
The program reads one context free grammar
and then it repeatedly reads strings to determine whether
they or an initial segment are in the language defined by the grammar.
The grammar that is read at the beginning has to be
in accordance with the following grammar,
so this is a <EM>grammar</EM> for <EM>grammars</EM>:
<XMP>
grammar         ::=   production  [ ';' production ]  '.'
production      ::=   identifier  '='  expression
identifier      ::=   letter  [ letter | digit | underscore ]
letter          ::=   'A' .. 'Z' | 'a' .. 'z'
digit           ::=   '0' .. '9'
underscore      ::=   '_'
expression      ::=   term  [ '|' term ]
term            ::=   factor  [ factor ]
factor          ::=   identifier
                |  ''' any-printing-character (except '.')
                |  '(' expression ')'
                |  '[' expression ']'
                |  '{' expression '}'
</XMP>
Thus a grammar consists of one or more productions
separated by semicolons, and then a terminating period.
A production consists of an identifier followed by the equals
sign followed by an expression.
An identifier consists of a letter followed optionally
by more letters or digits or underscores,
up to a maximum of 16 characters in all,
any further ones are ignored.
Note that there is no simple way to express this in a grammar.
An expression consists of one or more terms
separated by the alternation symbol,
and a term consists of one or more factors
which are taken to be concatenated.
A factor consists of either an identifier,
which will be taken to be a non-terminal defined somewhere in the grammar,
or a singly quoted printing character (but not a period),
which will be taken to be a terminal,
or an expression in (round) parentheses for overriding precedences,
or an expression in (square) brackets to be repeated zero or more times,
or an expression in (curly) braces to be repeated zero or one times.
<P>
A further restriction,
one which cannot be expressed by the above context free grammar,
is that any identifier which occurs as a factor must
occur on the left hand side of some production.
It does not matter whether the definition of that identifier
occurs before or after its first use in a factor.
<P>
During the reading of the program any context free errors are reported
in the form
<XMP>
        error: seen "X" when MESSAGE
</XMP>
where <CODE>X</CODE> is the last identifier or single character seen, and
<CODE>MESSAGE</CODE> is one of the following:
<XMP>
no white space as terminal              no "." as terminal
this character is illegal               beginning of factor expected
")" expected                            "]" expected
"}" expected                            non-terminal for production
"=" expected in production              "." expected at end of grammar
</XMP>
There is one possible context sensitive error:
if after the reading of the grammar a non-terminal <CODE>N</CODE>
is found to be undefined, then
an error is reported in the form
<XMP>
        undefined non-terminal N
</XMP>
The first error found in the grammar causes the program to exit.
<P>
When the grammar has been read successfully,
the program is ready to read any number of strings,
each terminated by a period.
The strings may contain white space --- blanks, tabs and newlines,
but such white space is ignored.
A string may contain up to a maximum of 100 printing characters.
For each string that has been read an attempt is made
to parse any initial segment of the string,
including the whole string, in accordance with the grammar.
If there are successful parses of any initial segments,
the message <CODE> ... well-formed -</CODE> is written on an output line.
Then follow sequentially numbered lines each containing
first an initial segment, then a longer space,
and then the remainder, if any, of the input string.
If any one segment can be parsed in different ways,
then the segment will be printed out as often as it can be parsed.
On the other hand,
if there are no successful parses of any initial segment of the input string,
then the message <CODE> ... ill-formed</CODE>
is written on one line.
The program terminates when the end of the input file
has been reached, even if this occurs
during the reading of the grammar or during the reading
of an input string.
<H3>An example run</H3>
<P>
The following is the output from a run of the program.
The first half consists of the grammar as it was written out by
the program while reading it from the input file.
The second part consists of input strings and responses by the system.
The grammar is for a usable version of predicate logic.
It was chosen because the syntax
of the two quantifiers <CODE>(Ax)</CODE> and <CODE>(Ex)</CODE>,
pronounced <EM>For all x</EM> and <EM>There exists an x</EM>
where <CODE>x</CODE> is a variable,
makes the grammar nondeterministic:
whether such a sequence of symbols is a quantifier or
a parenthesised atomic formula <CODE>Ax</CODE> or <CODE>Ex</CODE>,
with predicate <CODE>A</CODE> or <CODE>E</CODE>,
can often only be determined much further to the right
in the formula in which they occur.
Another source of ambiguity is the character <CODE>v</CODE>
which can mean the disjunction symbol between any two terms,
or it can be  variable belonging to a predicate in an atomic formula.
So again the parser will have to be able to backtrack.
When designing languages,
one normally avoids the need for this,
it was chosen here to illustrate the need for backtracking
and to illustrate its implementation.
<XMP>
FORMULA         =  EXPRESSION  { ('> | '=) FORMULA };
EXPRESSION      =  TERM  [ 'v  TERM ];
TERM            =  FACTOR  [ '& FACTOR ];
FACTOR          =  PREDICATE  [ CONSTANT | VARIABLE ]
                |  '-  FACTOR
                |  '(  FORMULA  ')
                |  '(  ('A | 'E)  VARIABLE  ')  FACTOR;
PREDICATE       =  'A | 'B | 'C | 'D | 'E | 'F | 'G | 'H | 'I
                |  'J | 'K | 'L | 'M | 'N | 'O | 'P | 'Q | 'R
                |  'S | 'T | 'U | 'V | 'W | 'X | 'Y | 'Z;
CONSTANT        =  'a | 'b | 'c | 'd | 'e | 'f | 'g;
VARIABLE        =  'u | 'v | 'w | 'x | 'y | 'z.

P v (Q > -(R & S)).
 ... well-formed -
1:      P                v(Q>-(R&S))
2:      Pv(Q>-(R&S))
3:      Pv                (Q>-(R&S))

Gb v Hcd  >  Iabc.
 ... well-formed -
1:      G                bvHcd>Iabc
2:      Gb                vHcd>Iabc
3:      GbvH                cd>Iabc
4:      GbvHc                d>Iabc
5:      GbvHcd                >Iabc
6:      GbvHcd>I                abc
7:      GbvHcd>Ia                bc
8:      GbvHcd>Iab                c
9:      GbvHcd>Iabc
10:     Gbv                Hcd>Iabc

(Ax)(Fx v Gx).
 ... well-formed -
1:      (Ax)                (FxvGx)
2:      (Ax)(FxvGx)

(Ex)(Ay)(Ez)(Fx & Gxy & Hyz).
 ... well-formed -
1:      (Ex)                (Ay)(Ez)(Fx&Gxy&Hyz)
2:      (Ex)(Ay)                (Ez)(Fx&Gxy&Hyz)
3:      (Ex)(Ay)(Ez)                (Fx&Gxy&Hyz)
4:      (Ex)(Ay)(Ez)(Fx&Gxy&Hyz)

P = Q.
 ... well-formed -
1:      P                =Q
2:      P=Q

(Ab)(Cd).
 ... well-formed -
1:      (Ab)                (Cd)

a & b  >  c v d.
 ... ill-formed

a = b.
 ... ill-formed

(Ai & Bj  >  Cx).
 ... ill-formed

</XMP>
<P>
The second part of the above output consists of echoed strings
from the input file,
each followed by a verdict by the parser to say whether
a leading substring of the string was ill-formed or well-formed.
If there were one or more leading well-formed substrings
then they are written out, followed by a separating space,
followed by the remainder of the input string.
<H2>Designing the implementation</H2>
<P>
The program 1) reads one grammar,
and then 2) repeatedly reads strings to determine whether
they are in the language defined by grammar.
It is best to break up the implementation into these two components.
<H3>Reading and storing the grammar</H3>
<P>
The input grammar has to be read and stored in some
suitable internal form so that when a string is to be
processed, the parser can match up the string against the grammar.
Reading and storing the grammar
is a by now largely familiar process,
the only really new aspects concern
the handling of the non-terminals
which are user declared identifiers.
<P>
This part of the program falls naturally into five sections:
the scanner, the parser, the code generator,
the context sensitive check that all non-terminals are defined,
and a relatively minor code optimisation.
<P>
<EM>The Scanner</EM>:
The scanner has to skip leading white space characters
and then be able to recognise identifiers and the ten
different single character symbols required by the grammar.
<P>
To recognise identifiers,
up to 16 characters have to be read into an otherwise blank string,
and any additional characters are also read but they are not entered
into the string.
This part is independent of whether the strings are inbuilt
reserved words --- as they were in Chapter 7,
or whether they are user declared identifiers.
For the grammar processor to be written here,
they are always user declared,
and when a particular identifier is encountered the first time,
then it has to be remembered
to ensure that the same identifier always means the same thing.
The remembering is best done by a symbol table.
So, when an identifier is encountered,
then the table is searched to determine whether it is there already.
If it is not, then this occurrence is the first one,
and then the identifier is entered.
The number of non-terminals in a grammar
is not likely to be large,
so a linear search is quite adequate.
An efficient linear search method uses a "sentinel"
(see Wirth, 1976, p 13 and also in function <CODE>position</CODE>, pp 316, 327, 340):
To determine whether an identifier is in the table,
we put a copy of it in a special place at the end of the table,
then we step from the front of the table comparing
what is in the table with the identifier being searched for,
until a match is found --- and there is guaranteed to be a match,
at least at the end of the table.
If the match occurs at the special place at the end,
then this is because the identifier was not already in the
remainder of the table.
In that case the identifier is entered properly,
at the next free place in the table.
The scanner has to report back to the parser
that the symbol was an identifier --- in our case always
a non-terminal.
In addition it has to report the location --- old or new ---
of the identifier.
It is only in very simple cases,
like the grammar processor,
that the entire symbol table management can be handled by the scanner.
<P>
To recognise the ten different single character symbols,
the scanner just needs a <CODE>CASE</CODE> statement with ten cases,
assigning ten appropriate values to a global variable <CODE>sym</CODE>
that will be inspected by the parser.
The eleventh value of that variable is reported
when an identifier has been found.
This completes the scanner.
<P>
<EM>The Parser</EM>:
For the parser most steps are quite familiar.
However, since the non-terminal <CODE>grammar</CODE> is not called
recursively, and in fact it is called only once,
there is no need to have a parsing procedure by that name.
Instead the right hand side of the production
for the non-terminal <CODE>grammar</CODE> can be handled directly in the main program
by a loop to process one or more productions.
Furthermore,
since the non-terminal <CODE>production</CODE> is not called recursively
and in fact only once,
and since the code for such productions will not need to be linked
together,
there is no need for a parsing procedure for productions either.
So, the body of the loop just referred to
can be made to handle the parsing of the right hand side
of the production for the non-terminal <CODE>production</CODE>.
The loop handling productions
will be exited when there is no further semicolon
as a production separator.
At this point the main program should enforce a period,
as required by the grammar.
<P>
It is a good idea to allow instead of the period,
say, a question mark
--- as the eleventh single character symbol in the scanner:
it can be used during debugging to signal
that some internal reporting is to take place.
Useful debugging information is
1) a list of the non-terminals together with
the root node for their expression tree,
2) the entire code that has been generated for the grammar, and
3) during parsing of each input string
the node currently being executed.
This secret feature of the program was used while it was being developed,
but it is not used in the demonstration run shown earlier.
<P>
However, for the remaining non-terminals,
namely <CODE>expression</CODE>, <CODE>term</CODE> and <CODE>factor</CODE>,
it is necessary to have explicit parsing procedures
because of the recursion.
But these procedures are very similar to the corresponding ones
in Chapter 9.
Only the parsing procedure factor is a little more involved,
since a factor is either
a non-terminal (an identifier),
or a terminal (a quoted character),
or a parenthesised expression inside <CODE>()</CODE>,
or a bracketed expression inside <CODE>[]</CODE>
or a braced expression inside <CODE>{}</CODE>.
As far as the parsing is concerned,
there is no difference between parentheses, brackets or braces ---
as long as they are matched.
This completes the parsing of the input grammars.
<P>
<EM>Internal Code Generation</EM>:
It is best to make the internal code as simple and familiar
as possible --- as so often before the binary tree form
recommends itself.
The nodes of the tree can be binary nodes
for alternation and concatenation,
or unary nodes for repetitions and options,
and nodes for non-terminals and terminals.
Inside factor,
there is a kind of chicken and egg problem:
a factor might be part of a production
defining a non-terminal N,
and this particular factor is a call to the non-terminal M
which has not yet been defined,
so the code for it has not yet been generated.
So, for factors that are non-terminals
one can only generate a code node which records
the position of the non-terminal in the symbol table.
That position will always be known at this point,
even if this was the first occurrence of that non-terminal.
In all other respects the code for factors
is unproblematic:
for bracketed and braced expressions
the code to be generated is a unary
repetition node or a unary option node.
The code generation for terms and factors
is routine by now.
<P>
In the main program the body of the loop
which handles productions must contain a call to the parsing
procedure <CODE>expression</CODE>.
When this call has returned,
then the last code that has been generated will be the principal
node for that expression.
At this point that code has to be attached to the location
in the symbol table of the non-terminal
for the production currently being processed by the loop.
So, in the body of the loop,
when the non-terminal has been read,
its location in the symbol table has to be saved.
Then, when the expression has been read,
the code address of the last generated node
has to be placed into the symbol table
at the saved location.
<P>
This completes the initial code generation:
the symbol table should now contain
an entry for each non-terminal
and beside that an entry for the address
of the principal node of the expression for that non-terminal.
<P>
<EM>Checking whether all non-terminals are defined</EM>:
When the parsing and code generating is completed,
the symbol table should contain code for every non-terminal
in the grammar, but of course it might not.
This would happen if a non-terminal was used inside a factor
somewhere, but there was no production for it.
This error can only be detected at the end of reading the grammar,
after the terminal period <CODE>.</CODE> has been checked in the main program.
It is an easy matter now to step through the symbol table
to see whether for every non-terminal there is a non-zero entry
for the code.
If there is a zero entry, then that non-terminal has never been
defined in a production,
and the input grammar is erroneous.
This is a context-sensitive error which
could not be specified by the rules of the formal grammar
as given in the manual.
If such an error occurs,
the undefined non-terminal has to be reported
and a message given before program termination.
<P>
<EM>A minor code optimisation</EM>:
If the main program has reached this point,
every non-terminal entry in the symbol table
will contain a code entry alongside.
It would be possible to use the code
as it is, as follows:
the code that has been generated for factors
consisting of a call to a non-terminal
currently contains only the location of that non-terminal
in the symbol table,
because at the time such a factor was processed
the non-terminal might not yet have been defined.
To use this code,
the symbol table would have to be consulted
every time the code for that call is processed.
This is slightly inefficient,
so instead of the location in the symbol table
we could now use the code that is now known to be
associated with that non-terminal.
To do this, the main program can make a single
linear sweep through all the code
that has been generated,
and for each call of a non-terminal
we put the code address of the code for the non-terminal
into the call instruction itself.
There is no need to delete the original location in the instruction,
since there are two fields available in the instruction.
Indeed, if one wanted to,
the parser to be described in the next section
could be given a tracing facility
to write out the names of non-terminals
when they are being called.
<H3>Processing input strings</H3>
<P>
After reading and storing a grammar,
the program repeatedly reads input strings and determines
whether they or an initial segment are in the language
defined by the grammar.
Hence after the grammar has been read and checked by the main
program, a <CODE>REPEAT</CODE> loop is needed to process input strings.
The processing of the input strings falls naturally into
three parts:
1) the input string has to be read and stored
in some internal form,
2) the internal form of the string
together with the internal form of the grammar
have to be passed to a parsing routine, and
3) if the parsing of the string or an initial segment was
successful then this has to be reported in some way,
and if it was unsuccessful an error has to be reported.
<P>
<EM>Storing the input string</EM>:
For many likely uses input strings will be allowed to contain
white space characters which are not significant;
for example in the various languages we have seen so far
this is the case.
In programs that we have written for these languages
the scanning procedure <EM>getch</EM>
simply skipped white space characters.
The simplest solution for the present purposes
is to allow white space in the input strings,
but to store only the printing characters.
This will have the consequence that
any quoted white space in the grammar will never match.
The alternative, of forcing users to treat
white space explicitly in the grammar
is equally easy to implement,
but makes the writing of the grammar itself cumbersome
(see one of the exercises).
Therefore we shall store the input strings without white space
characters.
Since we now need an explicit terminator for the
input string, we select the period <CODE>.</CODE>.
<P>
To implement this,
we need to declare a string variable
of some reasonable size (say 100),
together with an integer variable which is the length of
the actual string that has been read.
Initially the length variable is set to 0;
then we repeatedly read printing characters
and put them into the string until
the last such character is the string terminator period <CODE>.</CODE>.
This completes the saving of the input string.
<P>
<EM>The parsing routine</EM>:
This is very similar to the expanding procedure
of the regular expression program in Chapter 9.
Indeed for the operators of concatenation, alternation,
repetition and option the cases are identical.
<P>
The first major difference concerns the atomic case of a terminal symbol.
In the regular expression expander the character encoded in
the instruction had to be written into the output buffer,
then the continuation procedure had to be called,
and upon return the character had to be removed from the output buffer
by decrementing the buffer pointer.
Here the situation is different:
we are not creating new strings in an output buffer
but we are testing existing strings in an input buffer.
So the character encoded in the instruction
has to be compared with the character
in the input buffer at the current pointer position.
If the two are identical,
then parsing can proceed:
the pointer is incremented,
the continuation procedure is called,
and when that returns the pointer is decremented again.
On the other hand, if the two characters in the instruction
and in the input buffer are not identical,
then nothing happens --- the continuation procedure is ignored
and the recursive call simply returns.
<P>
The second major difference concerns the case of non-terminal symbols.
These result in a recursive call of the parsing procedure,
just as in Chapter 7 a procedure call statement
resulted in a recursive call of the
interpreting procedure which executes statements.
So, for non-terminals the parsing procedure
has to call itself,
using as the parameter the code which
was originally stored in the symbol table
and which in the optimisation stage was put into the
calling instruction itself.
<P>
<EM>Reporting on the parse</EM>:
In the initial global call of the parsing procedure
the first parameter is the code associated
with the first non-terminal of the input grammar,
and the second parameter is a global procedure
which reports on the success of the parse.
This global procedure is a continuation procedure,
if it is ever called then an initial substring
of the input string is in the language defined by the input grammar.
A reasonable way to indicate this is by writing
the accepted part of the input string up to the current pointer position,
then a space,
and then the remainder of the input string.
This global procedure may be called several times,
but always indirectly as a continuation.
If it has not been called, then the main program
has to report that the input string is ill-formed.
To make this possible,
before the call to the parsing procedure
a counter is initialised to zero,
and upon return the main program can inspect the counter
to see whether it is still zero.
The reporting procedure increments this counter
every time it is called,
it is also useful to write out the value of the counter
before writing out the two parts of the input string.
<H2>The program</H2>
<P>
The following is the standard Pascal source program
for the context free grammar parser.
<XMP>
PROGRAM confre(input,output);
(*  parser for CONtext FREe grammars *)
(*  uses "continuation" procedures as parameters        *)

LABEL 99;

CONST
    alfalength = 16;
    emptyalfa = '                '; (* 16 spaces *)
    maxnonterminals = 50;
    maxcode = 1000;
    maxstring = 100;

TYPE
    alfa = PACKED ARRAY [1..alfalength] OF char;
    message = PACKED ARRAY [1..30] OF char;
    pointer = 0..maxcode;
    symbol = (badchar,query,nonterminal,quote,altern,equal,
        lparen,rparen,lbrack,rbrack,lbrace,rbrace,semcol,period);
    operator = (tsy,nsy,alt,cat,rep,opt);
    node = RECORD op : operator; a,b : integer END;
VAR
    ch,lastch : char;
    sym : symbol;
    al : alfa;
    table : ARRAY [0 .. maxnonterminals] OF
        RECORD name : alfa; adr : integer END;
    lasttable,location : integer;
    code : ARRAY [1..maxcode] OF node;
    lastcode : pointer;

    instring : ARRAY [1..maxstring] OF char;
    sp,sl : integer;

    tracing : boolean;
    i : integer;
    num_parses : integer;

PROCEDURE getch;
BEGIN
IF eof THEN GOTO 99;
IF eoln THEN BEGIN readln; writeln; ch := ' ' END
        ELSE BEGIN read(ch); write(ch) END;
END;

(* - - - - -   R E A D I N G   T H E   G R A M M A R   - - - - - *)

PROCEDURE error(mes : message);
BEGIN
write('error: seen "');
IF sym = nonterminal THEN write(al) ELSE write(lastch);
writeln('" when ',mes);
GOTO 99
END (* error *);

PROCEDURE getsym;
VAR k : integer;
BEGIN
WHILE ch &lt= ' ' DO getch;
IF ch IN ['A'..'Z','a'..'z'] THEN
    BEGIN
    sym := nonterminal; k := 0; al := emptyalfa;
    REPEAT
        IF k < alfalength  THEN
            BEGIN k := k + 1; al[k] := ch  END;
        getch
        UNTIL NOT (ch IN ['A'..'Z','a'..'z','0'..'9','_']);
    table[0].name := al;
    location := lasttable;
    WHILE table[location].name &lt> al
        DO location := location - 1;
    IF location = 0 THEN
        BEGIN
        lasttable := lasttable + 1;
        WITH table[lasttable] DO
            BEGIN name := al; adr := 0 END;
        location := lasttable
        END
    END
ELSE
    BEGIN
    lastch := ch; getch;
    CASE lastch OF
        '?' : sym := query;
        '=' : sym := equal;
       '''' : sym := quote;
        '|' : sym := altern;
        '(' : sym := lparen;
        ')' : sym := rparen;
        '[' : sym := lbrack;
        ']' : sym := rbrack;
        '{' : sym := lbrace;
        '}' : sym := rbrace;
        ';' : sym := semcol;
        '.' : sym := period;
        OTHERWISE
            BEGIN
            sym := badchar;
            error('this character is illegal     ')
            END
        END (* CASE *)
    END (* ELSE *)
END  (* getsym *);

PROCEDURE generate(o : operator; x,y : integer);
BEGIN
lastcode := lastcode + 1;
WITH code[lastcode] DO
    BEGIN op := o; a := x; b := y END
END; (* generate *)

PROCEDURE expression;
VAR left : pointer;

    PROCEDURE term;
    VAR left : pointer;

        PROCEDURE factor;
        BEGIN (* factor *)
        CASE sym OF
            nonterminal :
                BEGIN
                generate(nsy,location,0 (* fixed in main *) );
                getsym
                END;
            quote :
                BEGIN
                IF ch < ' ' THEN
                    error('no white space as terminal    ');
                IF ch = '.' THEN
                    error('no "." as terminal            ');
                generate(tsy,ord(ch),0);
                getch; getsym;
                END;
            lparen :
                BEGIN
                getsym;
                expression;
                IF sym = rparen THEN getsym ELSE
                    error('")" expected                  ')
                END;
            lbrack :
                BEGIN
                getsym;
                expression;
                IF sym = rbrack THEN getsym ELSE
                    error('"]" expected                  ');
                generate(rep,0,lastcode)
                END;
            lbrace :
                BEGIN
                getsym;
                expression;
                IF sym = rbrace THEN getsym ELSE
                    error('"}" expected                  ');
                generate(opt,0,lastcode)
                END;
            OTHERWISE
                error('beginning of factor expected  ')
            END (* CASE *)
        END; (* factor *)

    BEGIN (* term *)
    factor;
    IF sym IN [nonterminal,quote,lparen,lbrack,lbrace] THEN
        BEGIN
        left := lastcode;
        term;
        generate(cat,left,lastcode)
        END
    END; (* term *)

BEGIN (* expression *)
term;
IF sym = altern THEN
    BEGIN
    getsym;
    left := lastcode;
    expression;
    generate(alt,left,lastcode)
    END
END; (* expression *)

(* - - - - -   I N T E R P R E T E R   - - - - - *)

PROCEDURE show;
VAR i : integer;
BEGIN (* show *)
IF num_parses = 0 THEN writeln(' ... well-formed -');
num_parses := num_parses + 1;
write(num_parses:0,':',chr(9) (* tab *) );
FOR i := 1 TO sp - 1 DO write(instring[i]);
write('                ');
FOR i := sp TO sl - 1 DO write(instring[i]);
writeln
END; (* show *)

PROCEDURE parse(t : integer; PROCEDURE cp);


    PROCEDURE alsoright;
    BEGIN parse(code[t].b,cp) END;

    PROCEDURE sameagain;
    BEGIN parse(t,cp) END;

BEGIN (* parse *)
WITH code[t] DO
    BEGIN
    IF tracing THEN writeln(t,op,a,b);
    CASE op OF
        tsy : IF instring[sp] = chr(a) THEN
                BEGIN sp := sp+1; cp; sp := sp-1 END;
        nsy : parse(b,cp);
        cat : parse(a,alsoright);
        alt : BEGIN parse(a,cp); parse(b,cp) END;
        rep : BEGIN cp; parse(b,sameagain) END;
        opt : BEGIN cp; parse(b,cp) END;
        END (* CASE *)
    END (* WITH *)
END; (* parse *)

(* - - - - -   M A I N   - - - - - *)

BEGIN (* main *)
lastcode := 0; lasttable := 0; ch := ' ';
REPEAT (* read productions *)
    getsym;
    IF sym &lt> nonterminal THEN
        error('non-terminal for production   ');
    i := location;
    getsym;
    IF sym = equal THEN getsym ELSE
        error('"=" expected in production    ');
    expression;
    table[i].adr := lastcode;
    UNTIL sym &lt> semcol;
IF sym = query THEN tracing := true ELSE
    IF sym = period THEN tracing := false ELSE
        error('"." expected at end of grammar');
FOR i := 1 TO lasttable DO
    IF table[i].adr = 0 THEN
        BEGIN
        writeln('undefined non-terminal: ',table[i].name);
        GOTO 99
        END;
FOR i := 1 TO lastcode DO
    WITH code[i] DO
        IF op = nsy THEN b := table[a].adr; (* fixup *)
IF tracing THEN
    BEGIN
    writeln('non-terminals:');
    FOR i := 1 TO lasttable DO
        WITH table[i] DO writeln(name,' : ',adr:0);
    writeln('code:');
    FOR i := 1 TO lastcode DO
        WITH code[i] DO writeln(i:4, op:12, a:8, b:8);
    writeln
    END;
REPEAT (* read and parse strings *)
    sl := 0;
    REPEAT
        REPEAT getch UNTIL ch > ' ';
        sl := sl + 1; instring[sl] := ch;
        UNTIL ch = '.';
    writeln;
    IF sl > 1 THEN
        BEGIN
        sp := 1; num_parses := 0;
        parse(table[1].adr,show);
        IF num_parses = 0 THEN writeln(' ... ill-formed')
        END;
    UNTIL false;
99:
END.
</XMP>
<H2>Exercises and reading</H2>
<P>
<EM>Another input grammar</EM>:
The following is another input grammar,
this time for a small programming language,
followed by a few input strings.
<XMP>
STATEMENTSEQUENCE =
      STATEMENT  [ '; STATEMENT ] ;

STATEMENT =
       LETTER  ': '=  EXPRESSION
    |  'B 'E 'G 'I 'N  STATEMENTSEQUENCE  'E 'N 'D
    |  'I 'F  EXPRESSION  'T 'H 'E 'N  STATEMENT
        { 'E 'L 'S 'E  STATEMENT }
    |  'W 'H 'I 'L 'E  EXPRESSION  'D 'O  STATEMENT
    |  'R 'E 'A 'D  LETTER
    |  'W 'R 'I 'T 'E  EXPRESSION ;

EXPRESSION =
    FACTOR  [ BINARYOPERATOR FACTOR ] ;

FACTOR =
       LETTER
    |  DIGIT  [ DIGIT ]
    |  '(  EXPRESSION  ') ;

LETTER =
      'a | 'b | 'c | 'd | 'e | 'f | 'g | 'h | 'i |
      'j | 'k | 'l | 'm | 'n | 'o | 'p | 'q | 'r |
      's | 't | 'u | 'v | 'w | 'x | 'y | 'z ;

BINARYOPERATOR =
      '+ | '- | '* | '/ | 'M 'O 'D | '< | '> | '= | '< '= | '> '= ;
DIGIT =
      '0 | '1 | '2 | '3 | '4 | '5 | '6 | '7 | '8 | '9 .



i := 10;
WHILE i >= 1 DO
  BEGIN WRITE i * i; i := i - 1 END.

BEGIN
i := 10;
WHILE i > 0 DO
  BEGIN WRITE i * i; i := i - 1 END
END.

IF a = b THEN IF c = d THEN e := f ELSE g := h.
</XMP>
How many initial segments are accepted for each
of the three input strings, and what are they?
Where does the parser have to backtrack?
Where does the parser find two ways of parsing an input string?
<P>
<EM>A grammar written in itself</EM>:
The grammar given in the manual is a grammar for
a language for expressing grammars.
Can you modify the grammar in the manual
so that it is written in the same language which it describes?
Alternatively,
can you devise a different context free grammar
suitable for describing context free languages,
including the language it is written in?
<P>
<EM>A subset of English</EM>:
A grammar processor in which the terminals have to be single
characters is hardly the sort of tool one would use
to process a grammar of English.
Nevertheless, as an exercise,
write a context free grammar for a very small subset of English
and use the program to parse some candidate sentences.
The sentence <CODE>Time flies like an arrow</CODE>
is a classic example of structural ambiguity ---
can your grammar handle it?
<P>
<EM>White space in grammars</EM>:
Assume that you have two programs which read grammars
and then parse strings.
One of them is the one we have written,
the other saves white space characters in the input string
and allows them in the grammar.
For each of the two programs,
write a grammar for propositional logic with operator
precedences,
with white space allowed for readability.
Compare the two grammars that you have written for readability.
Compare the two programs (one fictitious) for efficiency.
<P>
<EM>No initial segments</EM>:
Modify the program so that a parse is successful
only  if the entire input string has been consumed.
<P>
<EM>Sets of characters</EM>:
It is very bothersome for people to write long alternatives
of characters in the grammar, such as <CODE>'a | 'b | 'c | 'd</CODE> etc.
It would be more convenient to be able to write a set
such as <CODE>{abcd}</CODE>,
which is taken to mean the alternation of the characters.
It is also very inefficient for the computer if
the parsing procedure has to go through what is effectively
a linear list of characters.
It would be more efficient if the occurrence of
a character, say <CODE>c</CODE> in the input string <CODE>"efgchi"</CODE>
could be tested by using a faster method for determining
whether it is in the set of characters <CODE>{abcd}</CODE>.
So there are two independent reasons for
extending the program.
Implement such an extension;
represent an alternation of characters by a <CODE>SET OF char</CODE>
in the tree representation.
<P>
<EM>Reading</EM>:
Sudkamp (1988, pp 82 - 86)
gives a non-recursive top down algorithm
for parsing in accordance with a context free grammar,
it uses an explicit stack to implement the backtracking.
For a very recent survey of parsing techniques,
see Grune and Jacobs (1990).
On pp 253 - 258 it contains the "Unger parser",
for context free languages,
which is very different from the backtracking parser
developed in this chapter.
<P>
<EM>A Language Generator</EM>:
The program in Chapter~9 was a generator,
the program in this chapter is a parser.
Modify this parser so that it becomes a generator
for a given context free grammar.
As for the regular expression expander of Chapter~9,
it will be necessary to impose a (preferably flexible)
length limit on the strings generated.
<P>
<EM>Grammar transformations</EM>:
We have used extended BNF notation for the input grammar
because it lends itself for efficient parsing.
In many theoretical expositions only a minimal notation
is used:
there are no options, no repetitions and no parentheses.
For each non-terminal there are one or more productions
whose right hand sides are alternatives,
and each right hand side consists of just a concatenation
of terminals and non-terminals.
Rewrite the program so that,
instead of reading an input grammar and then parsing input strings,
it reads a grammar and then writes out an equivalent grammar
in this minimal notation.
This is an example of a grammar transformation.
Alternatively, rewrite the program to accept grammars
in this minimal form and then parse strings as before,
but be prepared for some loss of efficiency (Why ?).
Other transformations that you might consider are from (extended) BNF
or from minimal notation
to one of the so-called standard forms:
Chomsky normal form and Greibach normal form.
Aho and Ullman (1977) is a now classic theoretical text
with chapters on context free grammars and on normal forms.
Hopcroft and Ullman (1979) is another comprehensive theoretical
text in the field.
<P>
<EM>Rewriting systems</EM>:
Grammars are only one species of a wide class of what are called
rewriting systems.
Some of these, like grammars,
are <EM>generating</EM> systems which start in an initial configuration
and eventually generate a string.
Others, usually called automata,
are <EM>accepting</EM> systems,
they start with a string and eventually reach
either an accepting or rejecting configuration.
Both kinds can be nondeterministic in the sense that
at any one time several moves or changes are possible.
For an overview of both kinds of systems,
see Salomaa (1985, Chapters 2 and 3).
Many of the systems described there
lend themselves to an implementation
using the backtracking techniques of the present
and the two previous chapters.
<P>
<EM>For Reflection</EM>:
Study the evolution of the procedure <CODE>getsym</CODE>
in most of the previous programs.
Notice how various capabilities have changed.
<H2>Proplog --- propositional Prolog</H2>
<P>
This sections describes a very special case of context free grammars
and its relation to the propositional fragment
of the programming language Prolog.
<H3>Generalising the use of grammars</H3>
<P>
Grammars define languages or sets of strings.
Context free grammars define a particular species of languages,
the context free languages.
Context free grammars may be used by a general parser
to determine whether a given string is in the language,
or they may be used by a general generator
to construct all or some of the strings in the language.
The program in this chapter was a general parser,
and one of the exercises invited you to modify it so that
it becomes a generator.
In the remainder of this chapter we shall look at grammars
which define two very special languages:
the empty language which contains no strings at all,
and the language which contains just the null string.
<P>
Grammars consist of one or more productions
which declare non-terminals.
Each non-terminal then defines a language or set of strings.
In grammars one of the non-terminals is normally singled out
as the starting non-terminal,
either explicitly or implicitly --- say, the first.
The language defined by the grammar is then taken to be the one
defined by the starting non-terminal.
But grammars can equally well be used without a fixed
starting non-terminal,
by selecting different non-terminals as required.
Hence, whether for parsing or for generating,
a specific non-terminal of the grammar can be specified
for different tasks.
One can go one step further:
instead of specifying a non-terminal which is defined in the grammar
one can specify an expression built up from several
non-terminals and even terminals,
and then require parsing or generating to be done for that.
For example, using the grammar of the first exercise,
one might specify
<XMP>
        STATEMENT  DIGIT  '!  ( FACTOR | '% )
</XMP>
as the starting expression for parsing or for generating.
Among the strings accepted or generated will be:
<XMP>
        x := y  8  !  123
        IF  a = b  THEN  c := d  3  !  %
</XMP>
and so on.
<P>
In the program for the general parser we have insisted that all
non-terminals must be defined.
If a non-terminal is being used somewhere as a factor
but has not been defined after the reading of the grammar is complete,
then this was taken to be an error.
Indeed, in normal usage of grammars an undefined non-terminal
serves no purpose at all;
more likely than not it is due to a typing error.
However, in what follows we shall allow undefined non-terminals
and make good use of them.
<P>
In grammars there are generally many terminal symbols,
and then they occur as factors inside expressions,
hence inside the right hand sides of productions.
In the regular expression expander of Chapter 9
we even allowed the null string, written as <CODE>0</CODE>, as a factor.
Recall that <CODE>0</CODE> has the property that for all strings S,
<CODE>0</CODE>S = S = S<CODE>0</CODE>,
and hence in particular that <CODE>00</CODE> = <CODE>0</CODE>.
It would have been easy to add the null string
as a possible factor to the general parser of this chapter.
Parsing the null string always succeeds,
so in procedure parse we might have had a further case:
<XMP>
                nul : cp;
</XMP>
In the next section we shall look at grammars in which
the null string is the <EM>only</EM> terminal.
<H3>A dramatic restriction on productions</H3>
<P>
Grammars in which there are no terminals at all can only generate
the empty language {} which does not contain any strings as members.
Grammars in which there are no proper terminals would seem quite useless,
but this is not so.
Let us now restrict the terminals of a grammar
to allow only the (improper) terminal, the null string, <CODE>0</CODE>.
Such a grammar can generate at most the language <CODE>{0}</CODE>
whose only member is the null string.
But note that this language is different from the empty language <CODE>{}</CODE>
which does not have any members at all.
<P>
Without further loss of generality we stipulate that the right hand
side of any production is either just the null string,
or it is an expression built up solely from non-terminals.
Here is an example:
<XMP>
        alpha  =  0;
        beta  =  0;
        gamma  =  alpha delta;
        epsilon  =  beta;
        zeta  =  eta | epsilon;
        theta  =  iota | gamma.
</XMP>
Note that delta, eta and iota are used but not defined,
this is allowed now.
Remember also that we are no longer assuming that there is a fixed
non-terminal which is the starting symbol.
Instead a starting symbol or a whole
expression has to be specified for generation.
<XMP>
expression                                              language
beta                                                    {0}
delta                                                   {}
theta                                                   {}
beta  zeta                                              {0}
gamma  | iota  |  theta                                 {}
alpha  epsilon  (zeta | beta | theta)                   {0}
</XMP>
<P>
Think of the expressions in the left column as questions about the grammar
--- "what is the language generated by this expression?",
and think of the right hand column as the answers to this question.
Since there are only two possible answers,
we can treat the questions
as being of the form:
"is the language generated by this expression
the language containing just the null string?".
Answers will no longer be <CODE>{0}</CODE> or <CODE>{}</CODE>,
but <CODE>yes</CODE> or <CODE>no</CODE> instead.
Note that in normal use of a grammar the interest is in the
typically complex languages generated,
the grammar itself is only a tool.
Here, however,
the interest is in the productions of the grammar itself.

<H3>A logical interpretation</H3>
<P>
The restricted grammars and the questions in the preceding
section can be given a surprisingly different interpretation.
There are two kinds of productions:
1) those which have the null string as their right hand side ---
these we now call <EM>facts</EM>.
2) those which have an expression built up from non-terminals
as their right hand side --- these we now call <EM>rules</EM>.
The idea is that facts state that something is known to be true,
and that rules state that something can be shown to be true
if the right hand side can be shown to be true.
A <EM>database</EM> is a collection of facts and rules.
Questions are interpreted as requests to show that something is true
--- a <CODE>yes</CODE> answer indicates that it can be shown,
a <CODE>no</CODE> answer indicates that it cannot.
In expressions the explicit alternation symbol <CODE>|</CODE>
is taken to mean OR,
the (implicit) concatenations are taken to mean AND;
there is no negation.
<P>
This is the essence of Proplog,
the propositional fragment of the programming language Prolog.
The fragment, its use and an implementation
are described in detail in Maier and Warren (1988 Part I).
To make the surface syntax identical to that of Prolog,
we make the following changes:
Productions are not separated by semicolons but
terminated with periods.
For productions that are facts the <CODE>=</CODE> and the <CODE>0</CODE>
are not written at all.
For productions that are rules the <CODE>=</CODE>
is replaced by a turnstyle <CODE>:-</CODE> to mean <EM>if</EM>.
In expressions, alternations are now called disjunctions
and written with an infix semicolon <CODE>;</CODE> to mean <EM>or</EM>.
Concatenations are now called conjunctions
and are written with an explicit infix comma <CODE>,</CODE> to mean <EM>and</EM>.
<P>
The following is a tiny database written in this form,
followed by several questions.
The entries to the database are preceded by a <CODE>+</CODE>,
there are two facts and four rules.
The questions are preceded by a <CODE>-</CODE>,
there are six questions, each followed immediately by a yes/no
answer.
<XMP>
+
Sun_is_shining.
Peter_is_happy.
Weather_is_nice  :-  Sun_is_shining, It_is_warm.
Peter_is_pleasant  :-  Peter_is_happy.
Mary_is_happy  :-  John_is_pleasant ; Peter_is_pleasant.
John_is_happy  :-  John_won_lottery ; Weather_is_nice.
-
Peter_is_happy.
 ... yes
It_is_warm.
 ... no
John_is_happy.
 ... no
Peter_is_happy, Mary_is_happy.
 ... yes
Weather_is_nice ; John_won_lottery ; John_is_happy.
 ... no
Sun_is_shining, Peter_is_pleasant,
  (Mary_is_happy ; Peter_is_happy ; John_is_happy).
 ... yes
</XMP>
Note that the database and the questions and answers are <EM>isomorphic</EM>
to the grammar and the expressions and languages of the previous section.
<P>
Proplog is less expressive than propositional logic,
just as Prolog is less expressive than predicate logic.
For example, Proplog cannot be used to demonstrate the validity
of the complex constructive dilemma:
<CODE>p v q, p > r, q > s, therefore s v r</CODE>.
In the same way, Prolog cannot be used to demonstrate the validity of:
<CODE>Everything is F v G, ALL F are H, ALL G are I, therefore
Everything is H v I</CODE>.
Nevertheless, Prolog is an immensely useful programming language,
especially in applications that require backtracking.
Its propositional fragment  Proplog illustrates its basic structure
and its origins in grammars.
An understanding of the difference between Proplog
and classical propositional logic
goes a long way towards explaining the difference between Prolog
and classical predicate logic.
<P>
<EM>Exercise</EM>:
Modify the general parser for context free grammars so that it
becomes a theorem prover for Proplog.
You will need to distinguish between adding new facts and rules
to the database on the one hand,
and asking questions on the other.
One way is to precede each question with a question mark.
Another is to have two modes:
entering mode and questioning mode,
together with some way of switching between the two.
For instance, <CODE>+</CODE> and <CODE>-</CODE> could be used for the switching,
as in the example given above.
<P>
<EM>Negation</EM>:
All Prolog systems have a form of negation;
however, it is different from the classical one.
For a given database,
if <CODE>p</CODE> can be proven inside an expression,
then <CODE>not p</CODE> cannot,
and if <CODE>p</CODE> cannot be proven, then <CODE>not p</CODE> can.
But there is no way of entering negative facts into the database.
However, if it is assumed that the database contains
all the positive facts and rules that are relevant,
then the absence of a fact or rule should be sufficient
to establish its falsity.
This assumption is called the <EM>closed world assumption</EM>,
and it makes Proplog's and Prolog's <EM>negation by failure</EM>
quite different from its counterpart in classical logic.
Note that negation of either kind
does not correspond to anything familiar in grammars.
Implement negation by failure in your
theorem prover for Proplog.
</body>
</HTML>
